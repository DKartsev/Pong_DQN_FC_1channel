# Pong DQN (FC + 1 канал)

Небольшой учебный проект по решению Atari Pong с помощью DQN в условиях ограничений:

- только полносвязная нейросеть (без CNN);
- только один канал входа (grayscale);
- параметры обучения подобраны под ограниченную вычислительную мощность.

## Цель задания

Решить задачу Pong с помощью DQN, используя только полносвязанные слои и один канал входных данных.

## Основной файл

- `Pong_DQN_FC_1channel.ipynb` - основной ноутбук с обучением и графиками.

## Что реализовано

- среда: `ALE/Pong-v5`;
- ограниченное пространство действий: `NOOP`, `UP`, `DOWN`;
- предобработка: grayscale, обрезка игрового поля, уменьшение размера, разность соседних кадров;
- DQN с целевой сетью и replay buffer;
- Double DQN target;
- интерактивный вывод в процессе обучения (reward и moving average).

## Запуск

1. [Откройте](https://colab.research.google.com/drive/1xPeBoYHXD9tEn29EM4KUyPqxnPfAVyx7?usp=sharing) `Pong_DQN_FC_1channel.ipynb` в Colab/Jupyter.
2. Выполните ячейки сверху вниз.
3. Следите за метриками в процессе обучения:
   - `Reward` (награда за эпизод),
   - `Avg20` (средняя награда за последние 20 эпизодов),
   - `epsilon`.

## Итог по текущим экспериментам

- На 80 эпизодах устойчивого обучения не достигнуто.
- Большинство эпизодов: около `-21 ... -20`.
- Редкие улучшения до `-19` и `-18` были, но не закрепились.
- Финальная усредненная метрика (`Avg20`) вернулась к уровню около `-21`.

## Ограничения и интерпретация

Результат ожидаем с учетом ограничений задачи (FC + 1 канал) и ограниченного бюджета обучения. Пайплайн работает корректно, но для устойчиво сильной политики потребуется существенно больше вычислительных ресурсов и/или более выразительная архитектура.

